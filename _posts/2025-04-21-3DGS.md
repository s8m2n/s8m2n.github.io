---
title: "3D Gaussian Splatting"
date: 2025-04-22 
categories: [Study, Paper Review]
tags: [3d gaussian splatting, 3d representation, nerf]  # TAG names should always be lowercase
description: 3D Gaussian Splatting for Real-Time Radiance Field Rendering.  

math: true
---

## Modeling 3D spaces
![3D_modeling](/assets/img/3DGS/3D_modeling.png)

3D 공간을 모델링하는 방법은 여러가지가 있다. 대표적으로 
- 부피를 가지는 픽셀인 Voxel을 사용하는 방법
- Mesh
- Point cloud

등이 존재한다[^1].

주로 사용하는 방식은 Point cloud였지만 point 들은 모든 공간을 나타내지 못하고 **point 와 point 사이의 공간들이 너무 Sparse 하고,** 점이 없는 공간은 표현력이 떨어지는 문제가 있었다. 따라서 등장한 방법이 **NeRF 모델**이다.

NeRF는 특정 위치 좌표와 시점을 입력을 하면 Ray를 쏘고 해당 Ray를 Continuos한 Function으로 implicit 하게 3차원 공간을 구성한다. 기존의 View Synthesis 분야의 SOTA 모델은 Mip-NeRF360 모델이다. 하지만 기존 NeRF는 몇가지 문제점을 가진다.
![nerf_model](/assets/img/3DGS/nerf_model.png)
_nerf model_
- MLP를 사용하여 3D 공간 좌표를 입력받아 Color와 Density를 예측함. 하지만 Ray를 쏘고 그 경로 상의 점들을 가지고 Sampling을 하기 때문에 많은 샘플을 필요로 하고 학습 속도가 매우 느림
- 또한 렌더링 속도가 느리기 때문에 실시간 렌더링이 불가능, inference 시에 시점에서 바라보는 모든 pixel 에 대해 추론하기 때문에 시간이 오래 걸린다.

![nerf_vs_3dgsl](/assets/img/3DGS/nerf_vs_3dgs.png){: width="500" height="300"}

이와 같은 배경 하에, 다시 Point Cloud를 사용하여 3차원 공간을 explicit한 다른 방법으로 표현해보고자 하는 노력으로 등장한 모델이 3D Gaussian Splatting 이다.\
앞서 언급했듯 기존의 point cloud의 문제점은 점들로 공간을 표현하면 너무 Sparse 해서 공간의 표현력이 떨어진다.

> 그렇다면 point의 부피를 키워보면 어떨까..? 3D Gaussian으로!
{: .prompt-info }

공간을 부피가 있는 3D Gaussian 으로 표현하면 point cloud의 문제점인 sparsity를 극복할수 있지 않을까?! ![3d_gaussianl](/assets/img/3DGS/3d_gaussian.png){: .right }

따라서 point cloud에서 얻은 point를 중심으로 하고 부피를 가지는 3D 가우시안 타원체를 사용한다. 이로써 공간을 조금 더 Dense하게 표현할 수 있게 된다. 그리고 그렇게 구성한 3D 가우시안 타원체를 특정 시점의 방향으로 2D 평면에 “Splat” 하여 2D 이미지를 합성한다. 이를 **3D Gaussian Splatting**이라고 한다.

{% 
  include embed/video.html 
  src='/assets/img/3DGS/bear_pointcloud.mp4' 
  title='Teddy bear pointcloud'
  muted=true
%}

## Pinhole Camera Models
3D Gaussian Splatting에 대해 더 구체적으로 알아보기 전에, 사전적으로 알아두면 좋을 pinhole camera model에 대해 알아보자[^2]. 


Pinhole Camera Model은 3차원 좌표를 평면의 2차원 좌표로 매핑하는 모델이다. 3차원 → 2차원 좌표 매핑은 크게 두 가지 스텝을 거친다. 
1. 3차원 좌표를 Depth가 1인 평면 좌표로 매핑한다. (meter 단위)
   ![pinhole_camera](/assets/img/3DGS/pinhole_camera.png){: width="500" height="300"}
3. meter 단위의 (u,v,1) 을 이미지 단위 pixel 단위 (x,y,1)로 변환한다.
   ![intrincsic_parameter](/assets/img/3DGS/intricsic_parameter.png){: width="500" height="300"}

위 두 스텝으로 3차원 공간을 pixel로 projection 하고 2D 이미지를 얻어낸다. 
![pinhole_camera_final](/assets/img/3DGS/pinhole_camera_final.png)


추가적으로 3차원 공간의 임의의 점은 _같은 점이라고 해도 보는 시점에 따라 기준이 달라진다_. 이 점을 반영하기 위해 기준시점에서 카메라 시점으로 선형 변환을 위한 Extrinsic Parameter를 정의한다. 
![extrinsic_parameter](/assets/img/3DGS/extrinsic_parameter.png)


## 3D Gaussian Splatting 
3DGS의 주요 Task는 Novel View Synthesis, N개 시점에서 입력된 이미지를 사용해 모델을 학습하고 새로운 시점의 2D 이미지를 합성하는 것이다. 그 구체적인 과정을 알아보자.

### Overall Process
3DGS의 전체적인 학습 과정은
1. SfM 모델로 point colud 형성
2. Gaussian 으로 3D 공간 모델링
3. 카메라 뷰를 입력해서 2D이미지를 프로젝션하고 Raterization으로 이미지를 얻고
4. Ground Truth와 Loss를 비교해서 학습하는 방식으로 이루어진다.

이를 그림으로 표현하면 아래와 같다. 

![overall_process](/assets/img/3DGS/overall_process.png)
_3DGS의 전체적인 학습과정_

![algo1](/assets/img/3DGS/algo1.png)
_Optimization and Densification Algorithm_

![algo2](/assets/img/3DGS/algo2.png)
_Tile-Based Rasterization Algtithm_

### Elements of a Gaussian 
우선 SfM 모델을 활용하여 3D 공간의 poin cloud를 얻는다[^2]. 하나의 3DGaussian 은 4가지의 구성요소를 가지고 이 요소들이 학습 파라미터가 된다. 

![parameters](/assets/img/3DGS/parameters.png)

각각의 구성 요소들을 알아보자.
* Mean(SfM), Positions
  * mean은 3D 가우시안 타원체의 중심으로 공간상의 위치를 표현한다. SfM에서 얻은 point들을 그대로 mean으로 사용한다.
* Covariance
  * Covariance는 타원체의 모양과 회전된 정도(회전량)을 나타내는 파라미터이다.
    * **S**: mean을 중심으로 어떻게 생긴 타원을 만들지...
    * **R**: 3D space 상에서 어떤 방향을 나타낼지
      
    $$
    \sum RSS^{T}R^{T}
    $$
    
    를 가지고 하나의 3차원 공간상의 타원체를 구성한다. 초기값은 isotropic 한 구이다.
* Color
  * 시점에 따라 달라지는 색을 고려하기 위해 각각의 타원체는 Color 정보를 가지고 있다. NeRF는 RGB 값으로 color를 표현한 반면 3DGS는 Spherical Harmonics라는 방법을 사용한다. 점 하나를 보는 방향에 따라 다른 색으로 표현하는 기법으로 harmonics들의 weighted sum으로 구성된다. 
따라서 3DGS에서 color를 구현하기 위해 harmonics들의 Coefficient들을 사용한다[^3].


* Opacity
  * Opacity는 불투명성을 나타내는 값으로 공간상의 투명한 정도를 조정한다. 예를 들어 연기, 수증기와 같은 물체는 불투명성이 낮지만 rigid 한 물체는 불투명성이 높을 것이다.

#### Spherical Harmonics 
Color 값 C는 함수 Y에 대한 가중치 k의 가중합(weighted sum)으로 계산된다. 함수 Y는 색상 팔레트 역할을 하며 각 팔레트에는 특정 가중치 값이 할당된다. 

![spherical_coefficients](/assets/img/3DGS/spherical_coefficients.png){: width="500" height="300"}

입력 각도에 따라 함수 Y는 서로 다른 색상 값을 출력하며 이러한 색상 값들은 가중치에 따라 조합되어 최종적으로 단일한 색상을 형성하게 된다.
3D 모델링에서는 각 light source마다 서로 다른 coefficient 값을 갖게 된다. 또한, 각도에 따라 색상 값이 $\sum w \cdot  x$ 의 수식으로 계산되므로 실시간 렌더링이 가능해진다. 여기서 light source별 coefficient 값은 3D 모델을 보다 정확하게 표현할 수 있도록 최적화 과정을 거쳐 결정된다.


### Projection of a Gaussian

![camera_projection](/assets/img/3DGS/camera_projection.png)

3DGS를 초기화했으면 각 시점에 대해 2D 평면으로 projection을 해서 2D 이미지를 표현하고 이를 Ground Truth 이미지와 비교해서 Loss를 구한다. 

1. 우선 mean 값에 대해서는 앞서 언급한 PinHole Model을 적용하여 3차원 공간의 점을 2D 평면으로 매핑한다. 
2. 어느 시점으로 보느냐에 따라 2D 평면상 타원체의 projection이 달라지므로 Covariance($\sum$)는 이를 고려해서 Extrinsic Parameter ($W$)를 이용한 기준 시점과 카메라 시점 사이의 선형 변환 작업을 거친다.\
그리고 Intrinsic parameter DK의 affine approximation의 Jacobian 행렬($J$)과 그 역행렬을 곱해서
3차원 공간의 Covariance($\sum$)를 2차원의 Covariance($\sum^{'}$)로 매핑할 수 있다.
3. 이때 2차원 평면상 하나의 픽셀 위에 여러개의 가우시안 프로젝션들이 겹쳐질 수 있다.\
**⇒ Addressing the Overlapping Gaussians**

![camera_projection_final](/assets/img/3DGS/camera_projection_final.png)



---

NeRF의 Ray에서는 Ray위의 여러개의 점들에 대해 각 점들의 Opacity와 Occlusion(앞의 점들에 의해 가려지는 현상)을 고려하여 각 점들의 weight를 계산한다. 

$$ 
C = \sum_{i=1}^{N} T_i \alpha_i c_i 
$$

특정 점의 불투명성 Opacity와 앞 점들의 occlusion, Color 값을 모두 더해 특정 시점에서 pixel의 color를 구한다.

$$ 
C = \sum_{i=1}^{N} T_i \left( 1 - \exp(-\sigma_i \delta_i) \right) c_i
$$

특정 점의 앞에 있는 점들에 대해 Density$(\sigma)$를 모두 더하고 역수를 취해 Occlusion을 구한다.\
→ 앞 점들의 투명한 정도를 나타낸다. 

$$ 
T_i = \exp\left( -\sum_{j=1}^{i-1} \sigma_j \delta_j \right) 
$$

---


반면 3DGS에서는 2D 평면 상 하나의 픽셀에 대해 유한개의 가우시안 프로젝션이 존재한다. 그리고 그 픽셀을 기준으로 3차원 공간 상의 가우시안의 거리를 구해 각 가우시안들이 겹쳐진 순서를 구할 수 있다.  

특정 가우시안의 불투명성(Opacity)와 앞 가우시안들의 불투명성을 weight로 삼고 특정 가우시안의 color값에 weight를 곱하고 더해서 특정 시점에서 pixel의 color를 구한다.

$$ 
C = \sum_{i \in \mathcal{N}} c_i \alpha_i \prod_{j=1}^{i-1} (1 - \alpha_j)
$$

특정 가우시안 앞에 있는 가우시안들에 대해 1에서 Opacity를 뺀 값\
→ 앞에 있는 가우시안들의 투명한 정도를 나타낸다. 

$$ 
T_i = \prod_{j=1}^{i-1} (1 - \alpha_j) 
$$


### Tile-Based Rasterization

![tile_rasterization](/assets/img/3DGS/tile_rasterization.png)

3D Gaussian Splatting(3DGS)은기존 NeRF의 Volumetric Ray Marching 방식 대신, 3D 가우시안을 직접 2D 이미지로 프로젝션하여 실시간 렌더링을 가능하게 했고 특히Tile-Based Rasterization 기법을 적용하여 GPU에서 병렬적으로 빠르게 처리하면서도 가시성(Visibility)을 효율적으로 결정한다[^4].
[NVIDIA에서 작성한 ray tracingf과 rasterization의 차이점](https://blogs.nvidia.com/blog/whats-difference-between-ray-tracing-rasterization/)


Tile-Based Rasterization은 아래와 같은 단계로 이루어진다. 
1. **화면을 타일(Tile) 단위로 분할**\
     전체 화면을 16*16 픽셀 크기의 작은 Tile로 나누고 그 안에 들어오는 가우시안만 사용한다. 타일로 나눔으로써 병렬성을 극대화하고 GPU에서 연산을 더 효율적으로 수행한다.
2. **3D 가우시안들의 2D 프로젝션**    
   각 3D 가우시안들을 위 projection에서 언급한 방법을 이용해 2D 이미지 좌표로 변환한다.    
3. **가우시안들의 Depth 정렬 (GPU Radix Sort)**\
   각 가우시안이 포함된 타일 내에서 깊이(pixel과 3차원 가우시안사이의 거리를 기준으로) 값을 기준으로 정렬한다. 이때 GPU Radix Sort를 사용하여 병렬적으로 수행한다. 
4. **Alpha Blending**\
   정렬된 가우시안들을 앞에서부터 하나씩 차례대로 [Alpha-Blending](https://dusruddl2.tistory.com/26)을 수행하여 최종 색상을 결정한다.


### Loss Function

$$ 
L \leftarrow \text{Loss}(I, \hat{I}) 
$$

$$ 
\mathcal{L} = (1 - \lambda) \mathcal{L}_1 + \lambda \mathcal{L}_{\text{D-SSIM}} 
$$

$$ 
\lambda = 0.2 
$$

Projection of a Gaussian 을 통해 구한 2D 이미지 값와 Ground truth 이미지를 비교하여 Loss를 계산한다. 그리고 BackPropagation으로 gradient 를 계산하면서 가우시안들의 4가지 요소를 최적화해 나간다. 

#### Adaptive Density Control 
> 저자들은 Adaptive Density Control이 가장 핵심적인 부분이라고 주장한다. 
{: .prompt-info }

![ADC](/assets/img/3DGS/ADC.png)
_Adaptive Densirt Control Algorithm_

Loss를 최적화 하는 과정 중간 중간에 Adaptive Density Control 과정을 추가한다. 

이 과정은 크게 두가지로 구분 지을 수 있다. 넘치는 부분을 제거하는 **pruning**과 부족한 부분을 채우는 **Densification**이다. 
* Pruning
  * 만약 특정 가우시안의 Opacity가 임계값보다 작다면: $\alpha<\epsilon$ : \
    → 해당 가우시안은 너무 투명한.. 즉 없는 것과 마찬가지로 간주하여 Pruning을 통해 가우시안을 제거한다.
  * 하지만 3DGS의 목적은 Point Cloud의 Sparsity를 해결하는 것인데 Pruning의 반복으로 오히려 다시 공간의 표현이 sparse 해지는 것을 방지하기 위해 Densification 과정을 추가한다. 
* Densification
  * Loss의 Gradient 값이 임계값보다 크다면 : \
    → 해당 가우시안이 그 공간을 제대로 표현하지 못한다고 본다. 너무 작게 분포하거나, 너무 크게 분포하는경우가 있다.
    
    1. **Under-Reconstruction**: 가우시안이 공간을 표현하기에 너무 작게 분포한다고 여겨지는 경우, Gradient 변화에 의해 이동할 mean의 다음 위치에 기존 위치의 가우시안을 복제하고 각각을 최적화한다.
    2. **Over-Reconstruction**: 가우시안이 공간을 표현하기에 너무 크게 분포할때, 가우시안의 PDF로 부터 랜덤하게 두 점을 samling 하고 각 가우시안을 최적화한다.
       ![reconstruction](/assets/img/3DGS/reconstruction.png)

이와 같은 과정으로 저자들은 기존 SOTA 모델 Mip-NeRF360에 비해 SSIM, PSNR에서 좋은 성적을 얻었고 특히 매우 커진 FPS로 인해 실시간 렌더링이 가능해졌다.\
![results](/assets/img/3DGS/results.png)

이후 Ablation study에서는 Adaptive Density Control 을 하지않은 경우, 등방형의 가우시안을 사용한 경우, SfM 적용없이 랜덤한 초기화를 한 경우, 시점에서 가까운 가우시안만 사용한 경우 등 다양한 실험을 진행했고 그 성능 차이를 확인했다.
![ablation](/assets/img/3DGS/ablation.png)

[^1]: [3D Representations 블로그 글](https://jungsoo-ai-study.tistory.com/51)
[^2]: [sfm에 대한 블로그](https://woochan-autobiography.tistory.com/944)
[^3]: [Spherical Harmonics에 대해 정리한 글](https://xoft.tistory.com/50)
[^4]: [Rasterization VS Ray Tracing](https://blog.naver.com/skkim12345/221351312903)


   
