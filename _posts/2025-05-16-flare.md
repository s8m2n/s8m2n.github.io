---
title: "FLARE"
date: 2025-05-16
categories: [Study, Paper Review]
tags: [] # TAG names should always be lowercase
# description: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views

math: true

# > tip, info, warning...
# {: .prompt-tip}

# ```
# def step_function(x):
#     y=x>0
#     return y.astype(np.int)
# ```
---
## FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views
Multi-View Image로부터 3D Scene을 Reconstruction하는 작업은 다양한 Computer Vision 분야의 핵심 과제이다. 기존의 Traditional Approaches들은 이 문제를 2 Stage로 접근한다. 
1.	SfM을 사용한 Camera Parameter를 추정하고
2.	MVS를 사용해 Dense Depth Map을 계산해 3D 모델을 생성하는 접근법을 주로 사용한다. 

하지만 이 방식은 SIFT[^1]와 같은 handcrafted feature 추출 알고리즘에 의존하며, SfM은 non-differentiable한 요소를 포함하고 있어 딥러닝 파이프라인에 end-to-end로 통합하기 어렵다. 또한 Sparse View와 Limited viewpoints에 취약해 실제 Scene에 적용하는 데 제약이 있다.

또한 전통적인 방식에서는 전체 3D scene을 직접 예측하거나 최적화해야 하기 때문에, 모델이 처리해야 할 공간적 범위와 학습해야 할 변수의 수가 많아지고, 학습 안정성과 수렴 속도 측면에서 큰 부담이 있었다. 

![fig1](/assets/img/flare/fig1.png)

저자들은 이러한 한계를 극복하기 위해, Uncalibrated Sparse Image를 입력으로 받아 Geometry, Appearance, Camera Parameter를 한 번에 추론하는 feed-forward, differentiable framework인 FLARE를 제안한다.

![fig2](/assets/img/flare/fig2.png)

**핵심 아이디어는 Camera Pose가 2D 이미지를 3D 공간 내의 시야 영역(viewing frustum)으로 제한함으로써, 이후 단계의 학습 복잡도를 줄이고 효율적인 3D 학습이 가능하게 하는 것이다.** 이를 위해 FLARE는 다음과 같은 세 단계로 구성된 cascade learning 구조를 갖는다. 특히 Camera Pose를 각 State 간 Proxiy로 사용하여 전체 구조를 Decompose할 수 있다. 


1. Camera Pose Estimation

![fig3](/assets/img/flare/fig3.jpg)

파이프라인은 Neural Pose Predictor를 통해 시작된다. 이 모듈은 여러 sparse-view이미지로부터 coarse한 camera pose를 직접 회귀한다. 각 pose는 해당 이미지가 바라보는 3D 공간 내의observation frustum을 정의하며, 이를 통해 모델이 관찰 가능한 공간을 명확히 설정하고, 이후 단계의 Geometric Structure 학습에 기하학적인 단서를 제공하여 학습 복잡도를 줄인다.

2. Geometry Prediction and Unification

![fig4](/assets/img/flare/fig4.jpg)

예측된 pose를 기반으로, transformer 기반의 구조가 각 뷰에 대한 camera-centric point map을 예측한다. 이 Point Map들은 3D scene의 부분 구조를 의미하며, 이후 Neural Scene Projector가 이들을 통합하여 하나의 coherent global geometry로 변환한다. 이 **two-stage 기하 예측 방식(camera-centric → global)**은 복잡한 장면에서도 더 안정적이고 왜곡이 적은 결과를 도출한다.

3. Appearance Modeling via 3D Gaussians

![fig5](/assets/img/flare/fig5.jpg)

전역 geometry가 완성된 후, FLARE는 DPT 기반 Transformer 구조와 VGGSfM 스타일의 CNN 모듈을 결합하여 각 포인트를 3D Gaussians로 회귀한다. 이 단계에서는 각 포인트의 위치, 크기, 불투명도, spherical harmonic coefficients를 포함한 appearance 정보를 예측하며, 최종적으로 고품질의 neural rendering을 가능하게 한다.

>FLARE는 위 세 단계를 완전 differentiable한 하나의 feed-forward pipeline으로 구성하여, 기존 SfM–MVS 방식의 비효율성을 극복하고 sparse-view 환경에서도 빠르고 정확한 3D 재구성을 달성한다.
{: .prompt-info}

본 논문에서 주장하는 Main Contribution은 다음과 같다.

* Uncalibrated sparse-view 이미지로부터 0.5초 내에 high-quality 3D Gaussian scene을 feed-forward 방식으로 재구성하는 효율적인 differentiable system을 제안함
* Camera pose를 proxy로 활용하여 3D 학습의 복잡도를 낮추는 새로운 cascade learning paradigm을 도입함
* Camera-centric point map → global geometry projector 구조의 two-stage geometry learning 방식으로 빠른 수렴과 높은 정밀도의 geometry를 달성함

## Method
FLARE에서는Neural Network와의 Compatibility와 이후 3D Gaussian 적용의 용이성이라는 장점을 사용하고자 Point Map을 사용하여 Geometry Representation을 한다. 

여기서 **Point Map**이란 각 Camera coordinate에서 보이는 2D 이미지에 대해, 픽셀단위로 예측한 3D 위치(x,y,z) 좌표를 담은 map이다.** 다시 말해 3차원 공간상의 점의 좌표를 대응되는 2D map에 mapping 한 것으로 볼 수 있다. 

![fig2](/assets/img/flare/fig2.png)

위 구조에 나온 것처럼 우선 Sparse 한 input 이미지를 **Neural Pose Predictor**에 입력하여 Coarse한 View Pose를 일차적으로 예측한다. 그리고 Coarse Pose를 사용해 **Camera Point 기반의 Geometry**를 예측하고 이를 통합하여 **Global 한 Geometry를 예측한다.**이렇게 얻은 **예측된 Geometry로 3D guassian을 reconstruct 한다.** 그 구체적인 과정을 알아보자. 

### Neural Pose Predictor 
전통적인 pose Estimation 기법들은 Feature Matching에 의존하여 View 간 일치하는 point들을 찾는다. 하지만 이 방식은 Image들 간 overlapping이 거의 없는 Sparse Image에 대해서는 용하기 어렵다. 

따라서 저자들은 Feature Matching 기법을 과감하게 버리고 Transformer 모델을 활용하여 입력된 Sparse-View Image에서 Coarse한 Camera Pose를 얻는 방법을 도입한다. 

![fig4](/assets/img/flare/fig4.png)

입력 이미지 $\mathcal{I} = \{ \mathbf{I}_i \}_{i=1}^N$에 대해 각 이미지들을 Non-Overlapping patch를 이용해 Tokenize한다. 이때 이미지를 Tokenize하는 인코더는 pretrained 된 CroCo Encoder를 사용한다고 한다. 

그리고 학습 가능한 초기화된 Camera Latent $\mathcal{Q}_c = \{ \mathbf{q}_i^{\text{coarse}} \}_{i=1}^N$ 를 앞서 구한 Image Token에 Concatenate 해 1D Sequence를 생성하고Decoder Only Transformer: Neural Pose Predictor $F_p(\dot)$ 에 입력하여 Coarse 한 Camera Pose를 얻는다. 

$$
\mathcal{P}_c = F_p(\mathcal{Q}_c, \mathcal{I}) 
$$

$$
\mathcal{P}_c = \{ \mathbf{P}_i^{\text{coarse}} \}_{i=1}^N
$$

이때 출력값은 3D의 Transition Vector와 4D Quaternion Vector로 구성된 7차원 벡터로 표현된다. 

Neural Pose Predictor $F_p()$의 구체적인 구조는 아래와 같다.

![fig3](/assets/img/flare/fig3.png)

저자들은 한 가지 흥미로운 사실을 강조한다. **FLARE에서 예측된 coarse한 camera pose는 ground truth와 정확히 일치할 필요가 없이, 대략적인 위치와 방향만 근사해도 충분하다는 것이다.**

이러한 관점은 FLARE의 핵심 구조인 cascade learning과 밀접하게 연결되는데 한 번에 정확한 pose를 예측하려는 것이 아니라 우선 coarse한 pose를 예측한 뒤 이를 geometry 학습을 위한 prior로 활용한다는 전략이다.

따라서 FLARE는 pose의 정확도에 집착하지 않고 방향성만 파악되면 subsequent transformer가 더 빠르고 안정적으로 geometry를 학습할 수 있게 만든다. **결국 FLARE에서 pose는 예측 대상(output)이 아니라 geometry 학습을 구조화(structure)하기 위한 열쇠(key)로 활용되는 셈이다.**


## Multi-View Geometry Estimation
다음 단계에서는 앞서 얻은 예측된 Camera Pose를 이용해 Geometry를 예측하는 과정이다. 저자들은 2 stage로 이 과정을 접근한다.
1.	Local 한 Camera-Coordinate System에서 Camera Centric Geometry를 우선 학습하고 
2.	앞서 구한 Pose를 가이드로 삼아 Neural Scene Projector를 통해 Global World Coordinate System으로 통합한다. 

### Camera-centric Geometry Estimation
우선 Camera-Centric Geometry Estimation을 통해 각 Camera 시점에서 보이는 structure에 대해서만 geometry를 학습하기 때문에 전체적인 학습을 더욱 쉽게 할 수 있다. 

![fig5]/assets/img/flare/fig5png)

이미지 토큰 $\mathcal{I} = \{ \mathbf{I}_i \}_{i=1}^N$을 앞서 구한 coarse 한 pose Estimate $\mathcal{P}_c = \{ \mathbf{P}_i^{\text{coarse}} \}_{i=1}^N$와 Concatenate 하고 추가적인 학습 가능한 Pose Token $\mathcal{Q}_f $도 함께 Concatenate 하여 또 다른 Transformer 구조 $F_l(\dot)$ 에 입력한다. 이때 추가적인 $\mathcal{Q}_f $를 학습함으로써 Coarse 한 Camera Pose를 더 Fine하게 조정할 수 있다. 

$$
\mathcal{T}_l, \mathcal{P}_f = F_l(\mathcal{I}, \mathcal{P}_c, \mathcal{Q}_f)
$$

* $\mathcal{T}_l = \left\{ \mathbf{T}_i^{\text{local}} \right\}_{i=1}^{N}$ : 은 Local Point Token으로 patch 단위로 생성된 ray 상의 3D feature point이다. 정확한 위치는 모르지만 가능성 있는 위치에 존재한다고 가정한다.
* $\mathcal{P}_c = \{ \mathbf{P}_i^{\text{coarse}} \}_{i=1}^N$ : 는 Refined된 Pose Estimate 이다. 

이렇게 얻은 local Point Token은 다시 DPT-based Transformer[^2]에 입력되어 Spatial Upsampling을 거쳐 Dense 한 point map$\mathcal{G}_l = \left\{ \mathbf{G}_i^{\text{local}} \right\}_{i=1}^{N}$과 Confidence Map $\mathcal{C}_l = \left\{ \mathbf{C}_i^{\text{local}} \right\}_{i=1}^{N}$을 얻게 된다. 

$$
\mathcal{G}_l, \mathcal{C}_l = D_l(\mathcal{T}_l)
$$

*$\mathcal{G}_l = \left\{ \mathbf{G}_i^{\text{local}} \right\}_{i=1}^{N}$ : 은 각 ray 상에서 예측된 point의 3D 위치를 나타내는 point map이다. 
*$\mathcal{C}_l = \left\{ \mathbf{C}_i^{\text{local}} \right\}_{i=1}^{N}$ : 은 해당 위치 예측의 신뢰 정도를 나타낸다. 이후 global 한 Geometry를 계산할 때 weight처럼 사용된다. 

이해를 위해 쉽게 표현하자면 $\mathcal{G}_l$은 픽셀 별 Ray 상에서 가장 가능성이 높다고 생각하는 위치의 **Expectation** 값이고, $\mathcal{C}_l$은 그 예측이 얼마나 신뢰할 수 있는지를 나타내는 **Variance** 처럼 이해할 수 있다. 

또한 저자들은 이렇게 Geometry와 Camera Pose를 동시에 최적화 하는 Multi Task learning Scheme은 서로 보완하는 지도학습의 역할을 하여 학습 성능을 더 끌어올릴 수 있었다고 한다.[^3]

추가적으로 추론 과정에서 Pose를 잘못 추정하는 오류를 낮추기 위해 예측한 Camera Pose에 랜덤한 가우시안 Noise를 추가하는 Pose Augmentation 기법을 도입했다고 한다. 


[^1]: [SIFT알고리즘]( https://velog.io/@kowoonho/SIFT-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)

[^2]: [DPT 모델 리뷰]( https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dpt/)

[^3]: [PF_LRM 모델, Pose & Shape Joint Prediction]( https://totoro97.github.io/pf-lrm/)

