---
title: "FLARE"
date: 2025-05-16
categories: [Study, Paper Review]
tags: [] # TAG names should always be lowercase
# description: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views

math: true

# > tip, info, warning...
# {: .prompt-tip}

# ```
# def step_function(x):
#     y=x>0
#     return y.astype(np.int)
# ```
---
## FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views
Multi-View Image로부터 3D Scene을 Reconstruction하는 작업은 다양한 Computer Vision 분야의 핵심 과제이다. 기존의 Traditional Approaches에서는 이 문제를 2 단계로 접근한다. 
1.	SfM을 사용한 Camera Parameter를 추정하고
2.	MVS를 사용해 Dense Depth Map을 계산해 3D 모델을 생성하는 접근법을 주로 사용한다. 

하지만 이 방식은 SIFT[^1]와 같은 handcrafted feature 추출 알고리즘에 의존하며, SfM은 non-differentiable한 요소를 포함하고 있어 딥러닝 파이프라인에 end-to-end로 통합하기 어렵다. 또한 Sparse View와 Limited viewpoints에 취약해 실제 Scene에 적용하는 데 제약이 있다.

게다가 전통적인 방식에서는 전체 3D scene을 직접 예측하거나 최적화해야 하기 때문에, 모델이 처리해야 할 공간적 범위와 학습해야 할 변수의 수가 많아지고, 학습 안정성과 수렴 속도 측면에서 큰 부담이 있었다. 

![fig1](/assets/img/flare/fig1.png)

저자들은 이러한 한계를 극복하기 위해, **Uncalibrated Sparse-View Image를 입력으로 받아 Geometry, Appearance, Camera Parameter를 한 번에 추론하는 feed-forward, differentiable framework인 FLARE를 제안한다.**

![fig2](/assets/img/flare/fig2.png)

**핵심 아이디어는 Camera Pose가 2D 이미지를 3D 공간 내의 시야 영역(viewing frustum)으로 제한함으로써, 이후 단계의 학습 복잡도를 줄이고 효율적인 3D 학습이 가능하게 하는 것이다.**\
이를 위해 FLARE는 다음과 같이 세 단계로 구성된 cascade learning 구조를 갖는다. 특히 Camera Pose를 각 State 간 Proxiy로 사용하여 전체 구조를 Decompose할 수 있다고 한다.


1. Camera Pose Estimation

  ![fig3](/assets/img/flare/fig3.jpg)
  
  파이프라인은 Neural Pose Predictor를 통해 시작된다.\
  이 모듈은 여러 sparse-view이미지로부터 coarse한 camera pose를 Direct Regression을 통해 얻는다. 각 pose는 해당 이미지가 바라보는 3D 공간 내의 observation frustum을 정의하며, 이를 통해 모델이 관찰 가능한 공간을 명확히 설정하고 이후 단계의 Geometric Structure 학습에 기하학적인 단서를 제공하여 학습 복잡도를 줄인다.

2. Geometry Prediction and Unification
   ![fig4](/assets/img/flare/fig4.jpg)
   
   예측된 pose를 기반으로, transformer Based Structure로 각 뷰에 대한 camera-centric point map을 예측한다. 이 Point Map들은 3D scene의 부분적인 구조를 의미하며, 이후 Neural Scene Projector가 이들을 **통합하여 하나의 coherent global geometry로 변환한다.** 이 two-stage 기하 예측 방식(camera-centric → global)은 복잡한 장면에서도 더 안정적이고 왜곡이 적은 결과를 도출한다.

3. Appearance Modeling via 3D Gaussians

  ![fig5](/assets/img/flare/fig5.jpg)
  
  Global geometry가 완성된 후, FLARE는 DPT 기반 Transformer 구조에서 얻은 Feature와 Pre-Trained VGG network에서 얻은 Feature를 결합하여 CNN 모듈에 입력하고 각 포인트를 3D Gaussians로 회귀한다.\
  이 단계에서는 각 포인트의 위치, 크기, 불투명도, spherical harmonic coefficients를 포함한 appearance 정보를 예측하며, 최종적으로 고품질의 neural rendering을 가능하게 한다.

>FLARE는 위 세 단계를 완전 differentiable한 하나의 feed-forward pipeline으로 구성하여 기존 SfM–MVS 방식의 비효율성을 극복하고 sparse-view 환경에서도 빠르고 정확한 3D 재구성을 달성한다.
{: .prompt-info}

본 논문에서 주장하는 Main Contribution은 다음과 같다.

* Uncalibrated sparse-view 이미지로부터 0.5초 내에 high-quality 3D Gaussian scene을 feed-forward 방식으로 재구성하는 효율적인 differentiable system을 제안
* Camera pose를 proxy로 활용하여 3D 학습의 복잡도를 낮추는 새로운 cascade learning paradigm을 도입
* Camera-centric point map → global geometry projector 구조의 two-stage geometry learning 방식으로 빠른 수렴과 높은 정밀도의 geometry를 달성

## Methods
FLARE에서는Neural Network와의 Compatibility와 이후 3D Gaussian 적용의 용이성이라는 장점을 사용하고자 Point Map을 활용하여 Geometry Representation을 한다. 

여기서 **Point Map**이란 각 Camera coordinate에서 보이는 2D 이미지에 대해, 픽셀단위로 예측한 3D 위치(x,y,z) 좌표를 담은 map이다. 다시 말해 3차원 공간상의 점의 좌표를 대응되는 2D map에 mapping 한 것으로 볼 수 있다. 

![fig2](/assets/img/flare/fig2.png)

위 구조에 나온 것처럼 우선 Sparse 한 input 이미지를 **Neural Pose Predictor**에 입력하여 Coarse한 View Pose를 일차적으로 예측한다. 그렇게 얻은 Coarse Pose를 사용해 **Camera-CentricGeometry**를 예측하고 이를 통합하여 **Global Geometry**를 예측한다. 최종적으로 이렇게 얻은 **예측된 Geometry로 3D guassian을 reconstruct 한다.** 각 과정을 자세히 알아보자. 

### Neural Pose Predictor 
전통적인 pose Estimation 기법들은 Feature Matching에 의존하여 View 간 일치하는 point들을 찾는다. 하지만 이 방식은 Image들 간 overlapping이 거의 없는 Sparse Image에 대해서는 적용하기 어렵다. 

따라서 저자들은 Feature Matching 기법을 과감하게 버리고 Transformer 모델을 활용하여 입력된 Sparse-View Image에서 직접적으로 Coarse한 Camera Pose를 얻는 방법을 도입한다. 

![fig4](/assets/img/flare/fig4.png){: 'w'=400, 'h'=300}

입력 이미지 $\mathcal{I} = \{ \mathbf{I_i} \}_{i=1}^N$ 에 대해 각 이미지들을 Non-Overlapping 하는 patch를 이용해 Tokenize한다. 이때 이미지를 Tokenize하는 인코더는 pretrained 된 CroCo Encoder를 사용한다. 

그리고 학습 가능한 초기화된 Camera Latent $\mathcal{Q_c} = \{ \mathbf{q_i}^{\text{coarse}} \}_{i=1}^N$ 를 앞서 구한 Image Token에 Concatenate 해 1D Sequence를 생성하고, Decoder Only Transformer: Neural Pose Predictor $F_p(\cdot)$ 에 입력하여 Coarse 한 Camera Pose를 얻는다. 

$$
\mathcal{P_c} = F_p(\mathcal{Q_c}, \mathcal{I}) 
$$

$$
\mathcal{P_c} = \{ \mathbf{P_i}^{\text{coarse}} \}_{i=1}^N
$$

이때 출력값은 3D의 Transition Vector와 4D Quaternion Vector로 구성된 7차원 벡터로 표현된다. 

Neural Pose Predictor $F_p(\cdot)$의 구체적인 구조는 아래와 같다.

![fig3](/assets/img/flare/fig3.png)

저자들은 한 가지 흥미로운 사실을 강조한다.\
**FLARE에서 예측된 coarse한 camera pose는 ground truth와 정확히 일치할 필요가 없이, 대략적인 위치와 방향만 근사해도 충분하다는 것이다.**

이러한 관점은 FLARE의 핵심 구조인 cascade learning과 밀접하게 연결되는데... 
> 한 번에 정확한 pose를 예측하는 것이 아니라 우선 coarse한 pose를 예측한 뒤 이를 geometry 학습을 위한 prior로 활용한다는 전략이다.
{: .prompt-tip}


다시 말해 pose의 정확도에 집착하지 않고 방향성만 우선 파악 된다면 Subsequent한 Transformer에서 더 빠르고 안정적으로 geometry를 학습할 수 있게된다는 것이다. **결국 FLARE에서 pose는 예측 대상(output)이 아니라 geometry 학습을 구조화(structure)하기 위한 열쇠(key, 논문 상에서는 'Proxies'라는 표현을 사용한다)로 활용되는 셈이다.**


### Multi-View Geometry Estimation
다음 단계에서는 앞서 얻은 예측된 Coarse Camera Pose를 이용해 Geometry를 예측하는 과정이다. 저자들은 2 stage로 이 과정을 접근한다.
1.	Local 한 Camera-Coordinate System에서 Camera Centric Geometry를 우선 학습하고 
2.	앞서 구한 Pose를 가이드로 삼아 Neural Scene Projector를 통해 Global World Coordinate System으로 통합한다. 

#### Camera-centric Geometry Estimation
우선 Camera-Centric Geometry Estimation을 통해 각 Camera 시점에서 보이는 structure에 대해서만 geometry를 학습한다. 제한된 View에 대해서만 학습을 진행하기 떄문에 이후 과정에서 전체적인 학습을 더욱 쉽게 할 수 있게 된다. 

![fig5](/assets/img/flare/fig5.png){: 'w'=400, 'h'=300}

이미지 토큰  

$$
\mathcal{I} = \mathbf{I_i}_{i=1}^N
$$ 

을 앞서 구한 coarse 한 pose Estimate 

$$
\mathcal{P_c} = \left\{ \mathbf{p}_i^{\text{coarse}}\right\}_{i=1}^N 
$$ 

와 Concatenate 하고, 추가적인 학습 가능한 Pose Token $\mathcal{Q_f}$ 도 함께 Concatenate 하여 또 다른 Transformer 구조 $F_l(\cdot)$ 에 입력한다. 

이때 추가적인 $\mathcal{Q}_f $를 학습함으로써 Coarse 한 Camera Pose를 더 Fine하게 조정할 수 있다고 한다. Camera Centric Geometry Estimation Network $F_l(\cdot)$의 구조는 다음과 같다.

![fig7](assets/img/flare/fig7.png)

$$
\mathcal{T}_l, \mathcal{P}_f = F_l(\mathcal{I}, \mathcal{P}_c, \mathcal{Q}_f)
$$

$$
\mathcal{T}_l = \left\{ \mathbf{T}_i^{\text{local}}\right\}_{i=1}^{N}
$$ 

은 Local Point Token으로 patch 단위로 생성된 ray 상의 3D feature point이다. 정확한 위치는 모르지만 가능성 있는 위치에 존재한다고 가정한다.

$$
\mathcal{P}_c = \{ \mathbf{P}_i^{\text{coarse}}\}_{i=1}^N
$$

는 Refined된 Pose Estimate 이다. 

이렇게 얻은 local Point Token은 다시 DPT-based Transformer[^2]에 입력되어 Spatial Upsampling을 거쳐  출력값으로 Dense 한 point map $\mathcal{G}_l = \left\{ \mathbf{G}_i^{\text{local}}\right\}_{i=1}^{N}$ 과 Confidence Map $\mathcal{C}_l = \left\{ \mathbf{C}_i^{\text{local}}\right\}_{i=1}^{N}$ 을 얻는다.

$$
\mathcal{G}_l, \mathcal{C}_l = D_l(\mathcal{T}_l)
$$

$$
\mathcal{G}_l = \left\{ \mathbf{G}_i^{\text{local}}\right\}_{i=1}^{N}
$$ 

은 각 픽셀의 ray 상에서 예측된 point의 3D 위치를 나타내는 point map이다.
  
$$
\mathcal{C}_l = \left\{ \mathbf{C}_i^{\text{local}}\right\}_{i=1}^{N}
$$

은 해당 위치 예측의 신뢰 정도를 나타낸다. 이후 global 한 Geometry를 계산할 때 weight처럼 사용된다. 

>쉽게 표현하자면 $\mathcal{G}_l$은 픽셀 별 Ray 상에서 가장 가능성이 높다고 생각하는 위치의 **Expectation** 값이고, $\mathcal{C}_l$은 그 예측이 얼마나 신뢰할 수 있는지를 나타내는 **Variance** 처럼 이해할 수 있을 것 같다.
{: .prompt-tip}

또한 이전 연구를 바탕으로 저자들은 이렇게 Geometry와 Camera Pose를 동시에 최적화하는 Multi Task learning Scheme은 서로 보완하는 지도학습의 역할을 하여 학습 성능을 더 끌어올릴 수 있었다고 한다.[^3]

추가적으로 추론 과정에서 Pose를 잘못 추정하는 오류를 낮추기 위해 예측한 Camera Pose에 랜덤한 가우시안 Noise를 추가하는 Pose Augmentation 기법을 도입한다. 

#### Global Geometry Projection
다음은 Camera Centric Point Geometry Estimation을 Global Geometry로 통합하는 과정이다. 이때 Learnable geometry Projector $F_g(\cdot)$를 사용해 앞서 예측한 Refined Pose $\mathcal{P}_f$ 를 바탕으로 Local Geometry를 Global 한 Geometry로 변환할 수 있다. 구체적인 구조는 다음과 같다. 

![fig8](assets/img/flare/fig8.png)

FLARE는 이러한 투영 과정을 단순한 기하학적 연산이 아닌, 학습 가능한 구조로 처리한다. 

이를 위해 $F_l(\cdot)$과 동일한 구조의 Transformer 구조 Geometry Projector $F_g(\cdot)$ 를 사용하며, 입력으로는 각 뷰의 Local Point Token $\mathcal{T}_l$ 과 refined pose $\mathcal{P}_f$ 를 함께 사용한다. 

이 모듈은 뷰 간 self-attention을 통해 cross-view correspondence를 스스로 학습하며, 최종적으로 world 좌표계 기준의 Global Point Map $\mathcal{G}_g$ 과 Confidence Map $\mathcal{C}_l$ 을 생성한다.

![fig6](assets/img/flare/fig6.png)

* $D_g(\cdot)$ : DPT based Upsampling Decoder 
* $\mathcal{G}_g$ : Global Point Map
* $\mathcal{C}_l$ : Global Confidence Map 

또한 이 과정에서는 연산 효율성과 정확도를 동시에 고려하기 위해, dense한 camera geometry $\mathcal{G}_l$을 직접 사용하는 대신, 더 압축된 표현인 Local Point Token $\mathcal{T}_l$을 사용한다.

이와 같은 학습 기반의 정렬 방식은 기존 방식처럼 local geometry에서 직접 global 구조를 예측하는 것보다 더 높은 강건성과 일반화 성능을 가지며, sparse view에서도 안정적인 global 3D structure 생성을 가능하게 만든다.

### 3D Gaussian for Appearance Modeling 

![fig9](assets/img/flare/fig9.png){: 'w'=400, 'h'=300}

앞서 예측한 global 3D geometry $\mathcal{G}_g$를 기반으로 각 포인트를 중심으로 하는 3D Gaussian representation을 초기화할 수 있다. 

이때 각 포인트는 위치만을 가지고 있으므로 Gaussian을 구성하기 위한 나머지 요소들 (예를 들어 opacity, rotation, scale, spherical harmonic coefficients (SH)) 을 예측해야 한다. 이를 위해 FLARE는 Gaussian Regression Head를 추가하고 Appearance modeling을 수행한다.

구체적으로, 먼저 사전학습된 VGG 네트워크를 사용해 입력 이미지에서 **시각적 feature $\mathcal{V}$**를 추출한다. 동시에 Geometry Projector $F_g(\cdot)$ 위에 추가된 DPT 기반 head를 통해 얻은 appearance feature $\mathcal{A}$를 함께 활용한다. 

이 두 feature $[\mathcal{V}, \mathcal{A}]$를 결합해 얕은 CNN 디코더인 $F_a(\cdot)$에 입력하고, 최종적으로 각 3D Gaussian에 대한 파라미터들을 회귀한다. 여기서 학습되는 Gaussian 파라미터는 위치를 제외한 opacity $\mathcal{O}$, scale $\mathcal{S}$, Rotation $\mathcal{R}$, 그리고 SH coefficients를 포함한다.

렌더링 단계에서는 예측된 geometry와 ground truth geometry 사이에 존재할 수 있는 scale inconsistency 문제를 해결하기 위해 좌표계 정규화 과정을 거친다. 모든 scene은 unit space로 정규화되며, 이때 사용되는 평균 스케일 계수는 다음과 같다:

$$
s = \text{avg}(\mathcal{G}_g), \quad s_{gt} = \text{avg}(\mathcal{G}_{gt})
$$

위 두 값은 각각 예측 geometry와 ground truth geometry의 평균 크기를 나타내며, 이를 기준으로 좌표계를 정규화한다. 이 과정은 geometry뿐만 아니라 Gaussian scale parameter $\mathcal{S}$, 그리고 novel view에서의 camera 위치 $\mathbf{p'}$에도 동일하게 적용된다. 이러한 정규화 과정은 모두 differentiable Gaussian rasterizer $R(\cdot)$ 상에서 수행된다.

![fig10](assets/img/flare/fig10.png)

최종적으로 렌더링된 이미지를 $\mathbf{I'}$라고 할 때, 전체 렌더링 과정은 완전히 미분 가능하며 따라서 ground truth 이미지와의 차이를 측정하는 reconstruction loss를 기반으로 하는 end-to-end optimization이 가능해진다. 

이로써 FLARE는 단순한 geometry 복원 뿐 아니라, high-quality한 appearance 표현까지 통합된 방식으로 학습할 수 있다.

### Training Loss
FLARE는 전체 파이프라인을 joint learning framework로 학습하며, 총 3개의 loss로 구성된 multi-task loss를 사용한다:
1.	Camera Pose Loss
2.	Geometry Loss
3.	Gaussian Splatting Loss
각 항목은 아래와 같이 구성된다.

1.	Camera Pose Loss $\mathcal{L}_{\text{pose}}$
   
  카메라 포즈는 coarse pose와 이후에 정제된 fine pose 두 단계로 예측된다. 각 단계에서 ground-truth pose와의 차이를 Huber loss로 계산한다.[^4] VGGSFM에서 사용하는 Pose Loss를 기반으로 식을 아래와 같이 세울 수 있다.

  ![fig11](assets/img/flare/fig11.png)

2.	Geometry Loss $\mathcal{L}_{\text{geo}}$
     
  Geometry Loss $\mathcal{L}_{\text{geo}}$는 DUSt3R 논문에서 제안된 방식과 유사한 confidence-aware 3D regression loss로 구성된다. 

  ![fig12](assets/img/flare/fig12.png)

  이 손실 항은 입력 이미지의 각 유효한 픽셀 $\mathcal(D)^i$ 에 대해 다음 두 가지 기준으로 오차를 계산한다.

  * 카메라 좌표계(camera-centric)에서 예측한 3D 포인트와 ground-truth 포인트 간의 유클리드 거리.
  * World 좌표계로 정렬된 global geometry 상에서의 3D 위치 오차. 

이 두 항은 각각 confidence score $C_{i,j}$와 결합되어 가중치를 적용받는다. 추가적으로 각 항에는 log confidence term $-\log C_{i,j}$이 포함되어 있어, 모델이 스스로 자신의 예측 신뢰도를 조절할 수 있도록 유도한다. 

결과적으로 모델은 local geometry와 global geometry를 동시에 일치시키도록 학습된다.

3.	Gaussian Splatting Loss $\mathcal{L}_{\text{splat}}$

  Gaussian Splatting Loss $\mathcal{L}_{\text{splat}}$는 세 가지 구성 요소로 이루어져 있다. 

  ![fig13](assets/img/flare/fig13.png)

* 예측 이미지와 ground-truth 이미지 간의 단순한 L2 손실
* VGG 네트워크를 기반으로 한 perceptual loss $\mathcal{L}_{\text{percep}}$ : 이미지의 고수준 특성 차이를 학습한다. 
* 예측된 depth와 monocular depth estimator가 제공하는 GT depth 간의 차이를 정렬하는 depth supervision loss: 예측된 depth는 scale과 shift를 정규화 계수 $\mathbf{W}, \mathbf{Q}$로 보정한 뒤 비교가 수행된다.

이 세 가지 항은 단순한 RGB 렌더링 품질을 넘어서, scene structure와 depth 표현까지 정확하게 학습되도록 유도하는 역할을 한다.

마지막으로 총 손실은 세 가지 항을 각기 다른 가중치로 더하여 계산되며, 수식은 다음과 같이 표현된다

![fig14](assets/img/flare/fig14.png)

여기서 $\lambda_{\text{pose}}, \lambda_{\text{geo}}, \lambda_{\text{splat}}$는 각 손실 항의 상대적인 중요도를 조절하는 하이퍼파라미터로 사용된다.

[^1]: [SIFT알고리즘]( https://velog.io/@kowoonho/SIFT-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)
[^2]: [DPT 모델 리뷰]( https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dpt/)
[^3]: [PF_LRM 모델, Pose & Shape Joint Prediction]( https://totoro97.github.io/pf-lrm/)
[^4]: [Huber-Loss]( https://ai-com.tistory.com/entry/ML-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98-Huber-Loss-Smooth-L1-Loss)


